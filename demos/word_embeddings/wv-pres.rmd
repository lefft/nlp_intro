---
title: "fun with word vectors"
subtitle: "background, demos, applications, ..."
author: "timothy leffel"
date: "feb28/2018"
output:
  ioslides_presentation:
    df_print: kable
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
```

```{r session_setup}
lefftpack::lazy_setup(); library("ggrepel"); source("util.r")
wd <- "~/BOOSH/_datasets/_word_embeddings/___prez/" # ; setwd(wd)

data_path <- function(f) paste0(wd, "../_output/word2vec/", f)    # wrap path
plot_path <- function(f) paste0(wd, "../_plots/", f)                 # wrap path


# exposes `dat` (300x43981)
load(data_path("pruned.word2vec-words_as_cols.rda"))

# exposes `vec_dists` (491536x3)
load(data_path("pruned.word2vec-words_as_cols-pairwise_distances_top1k.rda"))


# functions to check whether we have a word or pair of words available
word_available <- word_check_closure(dat)
pair_available <- pair_check_closure(vec_dists)

```


```{r}
## look at king/queen etc. 
mf_words <- c('king','queen','man','woman') # ,'boy','girl','prince','princess')
mf_splitvar <- c(rep(c('M','F'), times=length(mf_words)/2), "F","M")


derived_vecs <- list(
  queen_guess = wv('king') - wv('man') + wv('woman'),
  king_guess = wv('queen') - wv('woman') + wv('man'))

kingqueen_tsne_plob <- tsne_vector_plot(
  dat, mf_words, mf_splitvar, add_vecs=derived_vecs,
  perplexity="auto", seed=6933)

kingqueen_tsne_plob_realonly <- tsne_vector_plot(
  dat, mf_words, splitvar=rep(c('M','F'), times=length(mf_words)/2),
  perplexity="auto", seed=6933)


kingqueen_heatmap_plob <- vector_heatmap(dat, mf_words, interpolate=TRUE,
                                         add_vecs=derived_vecs)

```




## What are "word vectors"?


- an *embedding* for a word $w$ is a short sequence of numbers representing "the meaning" of $w$ -- or more accurately: "the distribution" of $w$ in a training corpus."
- with a sufficiently large corpus (millions of words), intuitive relationships between words are reflected directly in the numerical representations. 

<!-- Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip -->


### **in short**

- word vectors are a way to represent the meaning of a word using a sequence of numbers. 
- their content reflects the contexts in which the underlying words are used in a corpus. 


<br><br><br>
\*ambiguous, but usually refers to *word embeddings* 


## What are they good for?! 

famously, certain analogies can be derived: 

```{r echo=TRUE}
closest_word_vec(wv("king"), exclude=c("king","kings","kingdoms"))
closest_word_vec(wv("queen"), exclude=c("queen","queens","queendom"))
```


```{r echo=TRUE}
closest_word_vec(
  vec = wv("king") - wv("man") + wv("woman"),
  exclude = c("king","kings","man","men","woman","women"))
```


## What are they good for?!

similarity between sequences of numbers can be measured with euclidean distance/cosine similarity. 

```{r echo=TRUE}
# for vectors A,B of length n: 
#   >> take the sum from 1 to n of Ai*Bi over 
#      the product of the respective root sum of squares 
euc_dist <- function(v1, v2){
  sum(v1*v2) / 
    ((sqrt(sum(v1^2))) * (sqrt(sum(v2^2))))
}
```


## What are they good for?!

if words are just sequences of numbers, then we estimate word similarity in exactly this fashion!  

```{r echo=TRUE}
euc_dist(wv("queen"), wv("king") - wv("man") + wv("woman"))
euc_dist(wv("prince"), wv("princess") - wv("girl") + wv("boy"))
```


## What are they good for?!

you can also **discover** words that are similar to one you have in mind:

```{r echo=TRUE}
# notice anything about the similar words?? 
closest_n_words(word="dog", n=5, vec_grid=vec_dists)
```


## What are they good for?!

you can also **discover** words that are similar to one you have in mind:

```{r echo=TRUE}
closest_n_words(word="talk", n=5, vec_grid=vec_dists)
```




## Visualizing words in vector space (and in 2-d space)

#### visualize word meanings with a heatmap 


```{r}
kingqueen_heatmap_plob

```


## Visualizing words in vector space (and in 2-d space)

#### visualize word meanings with a heatmap 


```{r}
vector_heatmap(dat, mf_words, interpolate=FALSE, add_vecs=derived_vecs)
```




## Visualizing words in vector space (and in 2-d space)

#### visualize word meanings in 2-d space (some info is lost)


```{r}
# kingqueen_tsne_plob
kingqueen_tsne_plob_realonly
```


\*this plot was created using a dimensionality reduction technique called "t-distributed stochastic neighbor embedding" (T-SNE)






## Where do they come from?!

- an embedding for a word $w$ is a short ($n\approx \{25,\ldots,200\}$) sequence of real numbers that represents the meaning or distribution of $w$ 
- embeddings are derived by factorization of a very large term-term cooccurrence matrix 
- the resulting low dimensional matrix holds the "word vectors" -- e.g. with a 1000 word vocabulary, we would have a 1000x100 matrix containing a row for each word 
- basically, you do some dimensionality reduction magic to reduce the huge matrix into a more manageable one (some info necessarily lost)

## Most common models 

the most common frameworks for training word embeddings: 

- **word2vec** -- came from Google, see [[this seminal paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)] (Mikolov et al. 2013)
- **GloVe** -- came from Stanford, see [[paper](https://nlp.stanford.edu/pubs/glove.pdf)] and [[homepage](https://nlp.stanford.edu/projects/glove/)] (Pennington, Socher, and Manning 2014)


## Most common models 

common techniques for constructing input matrices are: 

- **CBOW** (continuous bag of words); and 
- **skip-gram** (used in word2vec) 

both involve counting co-occurrences of every pair of words in the vocab, across a moving window of fixed size


> The GloVe model is trained on the non-zero entries of a global word-word co-occurrence matrix, which tabulates how frequently words co-occur with one another in a given corpus. Populating this matrix requires a single pass through the entire corpus to collect the statistics. For large corpora, this pass can be computationally expensive, but it is a one-time up-front cost. Subsequent training iterations are much faster because the number of non-zero matrix entries is typically much smaller than the total number of words in the corpus.



<!-- - many procedures that construct large co-occurrence matrices and refactor them into a lower-dimensional matrix with a row for each word can be viewed as word embedding "models"  -->



## Some cool tricks 


- find words most similar to a given word (**got a word in mind, anyone??**)
- visualize the similarity of a set of words (**got a set of words in mind, anyone??**)
- combine vectors to "construct a word-meaning", and then see what *that* is similar to (**got a word-based analogy in mind, anyone??**)




## Cigarettes example 

```{r}
# words we want to ignore 
cig_words <- c(
  'cigarette','cigarettes','Tobacco','tobacco','Cigarette','Cigarettes')

```

```{r echo=TRUE}
closest_word_vec(wv('cigarette'), exclude=cig_words)
closest_word_vec(wv('cigarette') - wv('tobacco'), exclude=cig_words)
closest_word_vec(wv('cigarette') - wv('tobacco') + wv('marijuana'), 
                 exclude=cig_words)
```


## Cigarettes example

#### hypothesis: if we used twitter-trained embeddings, then we could derive the following: 

> `wv('cigar') - wv('tobacco') + wv('weed') `$\approx$` wv(blunt)` 


```{r echo=TRUE}
# but nothing much here :(
euc_dist(wv('cigarette')-wv('tobacco')+wv('marijuana'), wv('joint'))
euc_dist(wv('cigar')-wv('tobacco')+wv('marijuana'), wv('blunt'))
```




## Dogs/cats example 


```{r}
## visualize word similarities with vector heatmap 
hm_words <- c('dog', 'dogs', 'cat', 'cats', 'at')
(hm <- vector_heatmap(dat, words=hm_words, interpolate=TRUE))
# ggsave(plot=hm, 
#        file=plot_path('hm-2018-02-26.pdf'), width=8, height=3, units="in")
```

## Dogs/cats example 

```{r}
## visualize word similarities by reducing to 2d via tsne, then plotting 
# tsne_words <- c('dog', 'dogs', 'cat', 'cats', 'rabbit', 'rabbits')
tsne_words <- c('dog', 'dogs', 'cat', 'cats', 
                'mouse', 'mice', 'calf', 'calves')
tsne_splitvar <- sapply(tsne_words, function(w) grepl("mice|s$", w))

(sp <- tsne_vector_plot(dat, tsne_words, tsne_splitvar, perplexity='auto'))
# ggsave(plot=sp, file="_plots/tsne-02-26.pdf", width=6, height=4, units="in")


```



## Regular/irregular morphology example 


```{r}
freq_verbs <- as_data_frame(lefftpack::dataset_word_freq) %>% 
  filter(PartOfSpeech=='v') %>% arrange(Rank) %>% 
  top_n(n=30, wt=Frequency) %$% Word

freq_nouns <- as_data_frame(lefftpack::dataset_word_freq) %>% 
  filter(PartOfSpeech=='n') %>% arrange(Rank) %>% 
  top_n(n=30, wt=Frequency) %$% Word
```



```{r}
# top five verbs with regular and irregular past tense forms  
verbs <- data_frame(
  word = c(c('want','look','use','work','call'), 
           c('wanted','looked','used','worked','called'), 
           c('have','do','say','go','get'), 
           c('had','did','said','went','got')), 
  regular = rep(c(TRUE, FALSE), each=10), 
  feature = rep(c("tense_prs","tense_pst"), each=5, times=2)
)

# top five irregular-plural and regular-plural nouns 
nouns <- data_frame(
  word = c(c('time','year','way','day','thing'),
           c('times','years','ways','days','things'), 
           c('person','man','woman','life','child'), 
           c('people','men','women','lives','children')), 
  regular = rep(c(TRUE, FALSE), each=10), 
  feature = rep(c("number_sg","number_pl"), each=5, times=2)
)

words_df <- rbind(verbs, nouns)
```


```{r echo=TRUE}
str(words_df)
```


## Regular/irregular morphology example (verbs against nouns)


```{r echo=FALSE}
### plotting verbs against nouns 
tsne_words <- words_df$word
tsne_splitvar <- words_df$feature
tsne_vector_plot(
  dat, tsne_words, tsne_splitvar, perplexity='auto', 
  seed=6933, scale_colors=c("#ff1c1c", "#fc9292", "#1c4dff", "#91a8fc"))
```


## Regular/irregular morphology example (closer look at verbs)

```{r echo=FALSE}
### plotting regular against irregular verbs 
tsne_words <- verbs$word
tsne_splitvar <- paste(verbs$regular, verbs$feature, sep="_")
tsne_vector_plot(
  dat, tsne_words, tsne_splitvar, perplexity='auto', 
  seed=6933, scale_colors=c("#ff1c1c", "#fc9292", "#1c4dff", "#91a8fc")) + 
  labs(subtitle="vectors for regular and irregular past-tense verbs,\nprojected onto 2d space")
```











## Some project ideas 

- learn semantic composition by training 1-, 2-, ..., -gram vectors 
- a visual tool for keyword discovery and similarity measurement 
- train vecs on different subsets + compare (e.g. by time, as in post from last wk)
- ...












<!-- Both architectures describe how the neural network "learns" the underlying word representations for each word. Since learning word representations is essentially unsupervised, you need some way to "create" labels to train the model. Skip-gram and CBOW are two ways of creating the "task" for the neural network -- you can think of this as the output layer of the neural network, where we create "labels" for the given input (which depends on the architecture). -->

<!-- For both descriptions below, we assume that the current word in a sentence is wi. -->

<!-- CBOW: The input to the model could be wi−2,wi−1,wi+1,wi+2, the preceding and following words of the current word we are at. The output of the neural network will be wi. Hence you can think of the task as "predicting the word given its context" -->
<!-- Note that the number of words we use depends on your setting for the window size. -->

<!-- Skip-gram: The input to the model is wi, and the output could be wi−1,wi−2,wi+1,wi+2. So the task here is "predicting the context given a word". In addition, more distant words are given less weight by randomly sampling them. When you define the window size parameter, you only configure the maximum window size. The actual window size is randomly chosen between 1 and max size for each training sample, resulting in words with the maximum distance being observed with a probability of 1/c while words directly next to the given word are always(!) observed. (correction thanks to Christina Korger ) -->

<!-- According to Mikolov: -->

<!-- Skip-gram: works well with small amount of the training data, represents well even rare words or phrases. -->
<!-- CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words -->
<!-- This can get even a bit more complicated if you consider that there are two different ways how to train the models: the normalized hierarchical softmax, and the un-normalized negative sampling. Both work quite differently. -->

<!-- which makes sense since with skip gram, you can create a lot more training instances from limited amount of data, and for CBOW, you will need more since you are conditioning on context, which can get exponentially huge. -->

