{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classification Demo <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in data and reorganize a bit \n",
    "spam_data = '../sample_datasets/spam_clf/uci_ml_spam.txt'\n",
    "dat = pd.read_csv(spam_data, names=['ham_spam', 'text'], sep='\\t')\n",
    "\n",
    "dat['label'] = dat.ham_spam == 'spam'\n",
    "dat = dat[['ham_spam', 'label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam data dimensions: (5572, 3)\n",
      "  ham_spam  label                                               text\n",
      "0      ham  False  Go until jurong point, crazy.. Available only ...\n",
      "1      ham  False                      Ok lar... Joking wif u oni...\n",
      "2     spam   True  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  False  U dun say so early hor... U c already then say...\n",
      "4      ham  False  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# inspect the dataset \n",
    "print('spam data dimensions: {shape}'.format(shape=dat.shape))\n",
    "print(dat.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution of labels:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: ham_spam, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('distribution of labels:')\n",
    "dat.ham_spam.value_counts() # / sum(dat.ham_spam.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "# test-train split \n",
    "train_idx = sample(range(len(dat)), int(len(dat)*.75))\n",
    "test_idx = [n for n in set(range(len(dat))) - set(train_idx)]\n",
    "\n",
    "train_text = list(dat.text.iloc[train_idx])\n",
    "train_labs = list(dat.label.iloc[train_idx])\n",
    "\n",
    "test_text = list(dat.text.iloc[test_idx])\n",
    "test_labs = list(dat.label.iloc[test_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# preprocess text (cast to a count-based document-term matrix)\n",
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(1,1), \n",
    "                             stop_words='english')\n",
    "train_dtm = vectorizer.fit_transform(train_text)\n",
    "\n",
    "# check out the transformed data \n",
    "# vectorizer.get_feature_names()[999:1011]\n",
    "# list(vectorizer.get_stop_words())[0:10]\n",
    "# vectorizer.get_params()\n",
    "# pd.DataFrame(train_dtm.A, columns=vectorizer.get_feature_names()).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# model training \n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_dtm, train_labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation \n",
    "test_dtm = vectorizer.transform(test_text)\n",
    "\n",
    "predictions = clf.predict(test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out of the box accuracy: 0.986 (waow)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "accuracy = metrics.accuracy_score(test_labs, predictions)\n",
    "print(f'out of the box accuracy: {round(accuracy,3)} (waow)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  prediction                                               text\n",
      "0  False       False  U dun say so early hor... U c already then say...\n",
      "1  False       False  Even my brother is not like to speak with me. ...\n",
      "2   True        True  Had your mobile 11 months or more? U R entitle...\n",
      "3  False       False  I'm gonna be home soon and i don't want to tal...\n",
      "4   True        True  SIX chances to win CASH! From 100 to 20,000 po...\n"
     ]
    }
   ],
   "source": [
    "eval_df = pd.DataFrame([*zip(test_labs, predictions, test_text)], \n",
    "                       columns=('label', 'prediction', 'text'))\n",
    "\n",
    "print(eval_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted  False  True \n",
      "observed               \n",
      "False       1186      7\n",
      "True          13    187\n"
     ]
    }
   ],
   "source": [
    "# can also add `margins=True` \n",
    "conf_mat = pd.crosstab(eval_df.label, eval_df.prediction, \n",
    "                       rownames=['observed'], colnames=['predicted'])\n",
    "\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted  False  True \n",
      "observed               \n",
      "False      0.989  0.036\n",
      "True       0.011  0.964\n"
     ]
    }
   ],
   "source": [
    "# we can also view a confusion matrix with column-normalized values\n",
    "print(round(conf_mat / conf_mat.sum(axis=0), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the bad predictions \n",
    "print(eval_df[eval_df.label != eval_df.prediction].sort_values('label'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
