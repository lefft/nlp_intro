---
title: "Natural Language Processing"
subtitle: "a brief tour of the field and some demos"
author: "NORC Monthly Data Science Meeting"
date: "timothy leffel // [http://lefft.xyz](http://lefft.xyz) // march28/2018"
output: 
  ioslides_presentation:
    css: input/slides.css
    logo: input/NORC Logo Color PNG.png
    df_print: tibble
    widescreen: true
---


<!-- smaller: true -->

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo=FALSE, eval=TRUE)
```

## logistics

- most of today will be live demos
- clone this repository to follow along: [`https://github.com/lefft/nlp_intro`](https://github.com/lefft/nlp_intro)
- to run the code on your own machine, you'll need:
    - Python 3.3+, with `sklearn`, `pandas`, `numpy`
    - R 3.4+, with `dplyr`, `reshape2`, `Rtsne`, `ggplot2`, `ggrepel`, `gridExtra`


(dependencies also listed in the repo readme)




# outline 

## outline 

1. overview of NLP (10-15min)
    - what is natural language processing? (a bit of navel gazing)
    - common tasks and workflows 
2. modern tooling for NLP (5min)
    - languages and packages
3. live demos (40min)
    - document classification (Python)
    - word embeddings (R)


# 1. overview of the field of NLP

## what is natural language processing?

### disclaimer: no universally accepted definition! 

<br>

```{r, out.width="800px", fig.align="center"}
knitr::include_graphics("xkcd_114.png")
```



## what is natural language processing? (a linguist's take)

two branches of **computational linguistics**:

- symbolic computational linguistics: using computers to construct and evaluate **discrete** models of language structure and meaning -- the goal is to **understand** and **represent** human language 
- statistical computational linguistics: using computers to **learn** what actual language use is like by exposing them to **statistical regularities** that are present in **large, naturally-occurring corpora** 


## what is natural language processing? (a linguist's take)

> I think of Natural Language Processing as basically synonymous with "statistical computational linguistics"

> NLP is less of a unified field with an established *theoretical* tradition than it is an amalgamation of approaches to tasks that are derived from theoretical constructs from computer science, linguistics, and information theory. 



## what is natural language processing?

high-level distinction between **NLU** and **NLG**

- **Natural language understanding (NLU)**
- **Natural language generation (NLG)**

Usually when people talk about "natural language processing" they are talking about NLU. And more specifically, NLU **on text data** (i.e. not audio)

We'll focus on text-based NLU today 


## what is natural language processing?

easier to define NLP in terms of *tasks* and *objectives* than an abstract characterization 


some common tasks:

- document classification ("is this about a person or a place?")
- sentiment analysis ("is this text positive or negative?")
- topic modeling ("what are the topics in this set of documents?")
- named entity recognition ("who/what/where is this text talking about?")
- dependency parsing ("what is the structure of the sentences in this text?")
- and many many more...


## what is natural language processing?

hi-level tasks are broken down into discrete subtasks, e.g.

dependency parsing:

- tokenization
- lemmatization/stemming
- POS tagging
- build parse trees




<!-- ## NLP tasks (preprocessing) -->

<!-- important text-based NLU **preprocessing** tasks: -->

<!-- - tokenization (break up a corpus into words, phrases, sentences, etc.) -->
<!-- - lemmatization/stemming (reduce morphological variants to a common form, e.g. *running*/*ran*/*runs* $\Longrightarrow$ *run*) -->
<!-- - part-of-speech tagging (associate each token with a category like "noun", "verb", "preposition", etc.) -->
<!-- - parsing (turn raw text into a representation that approximates each sentence's internal structure) -->



## NLP tasks (general strategy/workflow)

- usually the unit of information in an NLP task is the "**document**" (e.g. a tweet or a webpage or an Amazon product review)
- a dataset is a set of documents, aka a **corpus**
- the name of the game is usually: 
    - do some preprocessing on the corpus (e.g. lowecasing, lemmatization); 
    - transform corpus into a numerical matrix (e.g. DTM, TCM); 
    - do a bunch of linear algebra over that matrix until you can either make the predictions you need to make, or view a visualization of the clustering you need to understand; and 
    - evaluate model predictions on the test/holdout set against human judgments



# 2. modern tooling for NLP 


## caveat 

- most NLP tasks are *very computationally expensive* (we'll see examples later)
- high-level languages like Python and R are great for exploring data and deploying small- to medium-scale models
- but when speed is crucial (e.g. question-answering), models are usually implemented in lower-level languages like Java or C++ (e.g. `word2vec` is implemented in C++; `GloVe` is implemented in Java)

Today we'll just look at modern tooling for Python and R

## Python or R? 

- as of 2018, the NLP ecosystem is much more mature in Python than in R
- R's memory management system makes it less than ideal for large-scale text analysis endeavors
- **BUT** recent NLP packages for R are `Rcpp::`-based extensions, and thus can overcome some of R's limitations (especially if you are using a high-performance machine or parallel processing) 

## Workflow 

My workflow is usually the following:

- use R to get a feel for a subset of the data (visualization, summary statistics) 
- use Python to develop + select a model 
- use Python to apply selected model to corpus (writing predictions to a file)
- use R to evaluate model performance, visualize, and report results (`.Rmd`/`ggplot2::`)

**NOTE**: if you haven't yet, check out R Studio's new `reticulate::` package -- makes it easy to integrate Python and R in a single workflow



## Python NLP tools (see also [PyPI index](https://pypi.python.org/pypi?%3Aaction=search&term=natural+language+processing&submit=search))

#### NLTK
- tried and true -- contains lots of corpora and datasets
- implements symbolic and statistical models (unique in this respect)

#### gensim
- implements bleeding edge models
- fast and efficient -- best for large-scale models

#### spacy
- nice API, easy to use, fast
- out of the box, *stuff just works* -- but not always super flexible

#### sklearn
- there are `sklearn` models designed for text -- all use the  `sklearn` API
- good starting point, and excellent for preprocessing

## R NLP tools (see also [CRAN NLP Task View](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html))

#### text2vec (still in beta!) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [[homepage](http://text2vec.org/)] [[CRAN](https://cran.r-project.org/web/packages/text2vec/index.html)]
- my favorite R package for NLP: fast and clean and lightweight
- main goal is to provide an interface for training term embeddings (implements `GloVe` and `word2vec` in `RCpp::`) -- but also provides nice `R6::` classes for key data structures like DTM's and TCM's

#### quanteda &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [[homepage](https://docs.quanteda.io/)] [[CRAN](https://cran.r-project.org/web/packages/quanteda/index.html)]
- a modern end-to-end framework for working with text data -- has a nice interface and seems like decent performance, but hasn't gained a lot of traction in the R community

#### tidytext &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  [[intro vignette](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)] [[CRAN](https://cran.r-project.org/web/packages/tidytext/index.html)] 
- excellent for learning, esp. with `dplyr::` and [Robinson & Silge (2016)](https://www.tidytextmining.com/)
- slow and therefore not a good choice for production-grade models


#### tm &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  [[intro vignette](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf)] [[CRAN](https://cran.r-project.org/web/packages/tm/index.html)]
- an older framework -- I haven't used it much (anyone??)

```{r}
lefftpack::dataset_word_freq
```




# 3. demos, demos, demos! 

## NLP project life cycle 

there are usually at least the following broad phases in an NLP project life cycle:

1. **data acquisition**: define and obtain or construct your corpus; set aside a subset of your data ($\approx$20-30%) for evaluation
2. **preprocessing**: regularize text; remove stop words and other junk; transform corpus into a format you can compute on; calculate some summary statistics do be used downstream (e.g. number of unique words, total word count)
3. **model development**: define your objective; select an algorithm that generates predictions you can use to measure accuracy relative to objective
4. **model evaluation + selection**: define a success metric (e.g. accuracy relative to human labels or some benchmark dataset)
5. **model "deployment"**: use model to generate predictions on unseen text. 




# demo 1: Naive Bayes spam filter

# demo 2: fun with `word2vec`




<link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Inconsolata">
<link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Montserrat">
<style>

pre {
  font-size: 12pt;
  width: auto;
  margin: auto;
  margin-left: 60px;
  font-family: 'Inconsolata', monospace !important;
  background-color: #f2f2f2;
  border: 1px lightgray solid;
  border-radius: 4px;
  padding: 4px;
}
code {
  font-family: 'Inconsolata', monospace !important;
}
table {
  width: 75% !important;
  margin: auto;
}

<!-- /* vertical-align: center !important; */ -->
.title-slide hgroup h1 {
  font-size: 48px;
  
}

<!-- /* ugh this is supposed to put small logo on subsequent slides but didnt */  -->
<!-- slides > slide:not(.nobackground):before { -->
<!--   width: 222px; -->
<!--   height: 54.5px; -->
<!--   background-size: 250px 75px; -->
<!-- } -->



</style>







